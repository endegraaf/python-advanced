{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe6ed68-eca1-46d7-9665-3a5f86f5d3a5",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28892af-56c3-44a1-8cf0-251af2439acb",
   "metadata": {},
   "source": [
    "## Wat gaan we doen? \n",
    "\n",
    "- Leren over diverse scraping libraries \n",
    "- Leren over werken met Browser Developer Tools (onderzoeken wat je wilt scrapen)\n",
    "- Werken met een aantal libraries vanuit Python\n",
    "- Aanroepen van een URL: GET, POST request\n",
    "- Parsen van data: (X)HTML en JSON\n",
    "\n",
    "We gaan eerst een klein stukje introductie doen en daarna aan de slag. \n",
    "\n",
    "\n",
    "\n",
    "## Introductie webscraping\n",
    "\n",
    "Dit notebook laat manieren zien hoe webpaginas te manipuleren zijn en hoe de data ervan te _scrapen_ is. \n",
    "\n",
    "> \"_Scrapen is een computertechniek waarbij software wordt gebruikt om informatie van webpagina's te extraheren en al dan niet te analyseren._\"  <sub>[wiki]<sub>\n",
    "\n",
    "In feite maak je contact via Python om informatie op te halen van een externe informatiebron zoals een website of api om daar gegevens vandaan te halen. Deze gegevens worden vervolgens in de eigen omgeving opgeslagen. \n",
    "    \n",
    "### Libraries\n",
    "Python heeft een aantal [internet] tools die daarbij helpen.  \n",
    "De meest gebruike zijn [urllib.request] en [html.parser].  \n",
    "Deze libraries hebben een _low-level_ design waardoor het te customizen is.  \n",
    "Hierdoor moet er een hoop code schrijven om de functies zo te maken dat het doel wordt bereikt.  \n",
    "Gelukkig zijn er third-party libraries die een _high-level_ interface bieden.  \n",
    "Python heeft ook de [json] module om objecten te encoden naar json en inkomende json te decoden.  \n",
    "\n",
    "Er zijn veel Third-party libraries de gebruikt kunnen worden om data van het web te halen.  \n",
    "Hieronder een kleine selectie waarvan we een paar in dit notebook gaat gebruiken.  \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f019bce-e422-4e50-bd65-a7c3384a520d",
   "metadata": {},
   "source": [
    "## Scrapen van een API\n",
    "\n",
    "Het makelijkste om data op te vragen is doormiddel van een API. Veel websites hebben geen openbare API.\n",
    "\n",
    "Toch kan het zo zijn dat de website een API gebruikt om data van de client naar de server te sturen.\n",
    "Zo kan de website dynamisch _(zonder te refreshen)_ de data op de pagina vernieuwen.   \n",
    "Hier wordt [XMLHTTPRequests (XHR)] voor gebruikt.\n",
    "\n",
    "In de Developer Tools onder de tab Network zie je het dataverkeer en de daarbij horende HTTP request methods.  \n",
    "Om goed te zien wat een website krijgt en verstuurd moet de optie Preserve log/ Persist Logs en de Disable Cache aan staan.  \n",
    "Met deze opties aan behoudt de network tab alle logs en vraagt de website alle data op die anders werd onthouden in de Cache.  \n",
    "\n",
    "\n",
    "\n",
    "[XMLHTTPRequests (XHR)]: https://nl.wikipedia.org/wiki/XMLHTTP\n",
    "[requests]: https://docs.python-requests.org/en/master/index.html \"Requests: HTTP for Humans\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b72f1b",
   "metadata": {},
   "source": [
    "#### Third-party libraries \n",
    "    \n",
    "[requests] is de _high-level_ interface om [HTTP request methods] te versturen naar API's of websites.\n",
    "\n",
    "[beautifulsoup4] wordt gebruikt om HTML of XML te parsen.  \n",
    "\n",
    "[selenium], de meest bekende web-automation tool welk ook veel gebruikt wordt om websites te testen.  \n",
    "\n",
    "[scrapy] een web-crawler framework voor Python met een hoop features.\n",
    "\n",
    "[httpx] de _next-generation_ HTTP client voor Python.  \n",
    "Als je bekend ben met [requests] dan is de overstap naar [httpx] klein.  \n",
    "Ondersteunt HTTP2 en de Python [async] functionaliteit [requests] ondersteunt dit niet.\n",
    "\n",
    "\n",
    "    \n",
    "[internet]: https://docs.python.org/3/library/internet.html \"Internet Protocols and Support\"\n",
    "[urllib.request]: https://docs.python.org/3/library/urllib.request.html \"Extensible library for opening URLs\"\n",
    "[json]: https://docs.python.org/3/library/json.html \"JSON encoder and decoder\"\n",
    "[html.parser]: https://docs.python.org/3/library/html.parser.html \"Simple HTML and XHTML parser\"\n",
    "\n",
    "[httpx]: https://www.python-httpx.org/ \"A next-generation HTTP client for Python\"\n",
    "[scrapy]: https://scrapy.org/ \"Web Crawling Framework\"\n",
    "[selenium]: https://www.selenium.dev/documentation/en/ \"selenium\"\n",
    "[beautifulsoup4]: https://beautiful-soup-4.readthedocs.io/en/latest/ \"Beautiful Soup\"\n",
    "[requests]: https://docs.python-requests.org/en/master/index.html \"Requests: HTTP for Humans\"\n",
    "\n",
    "[wiki]: https://nl.wikipedia.org/wiki/Scrapen\n",
    "[HTTP request methods]: https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods \"HTTP request methods\"\n",
    "[async]: https://docs.python.org/3/library/asyncio.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8caf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
